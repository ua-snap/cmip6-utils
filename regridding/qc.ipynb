{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bd18f6-e9e5-43e0-b8a7-73817fdfb282",
   "metadata": {},
   "source": [
    "# Quality control for regridding efforts\n",
    "\n",
    "Use this notebook to check the quality of the regridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ab1013-3464-40bb-95bc-03f7f607193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from config import *\n",
    "from regrid import generate_regrid_filepath, open_and_crop_dataset\n",
    "from crop_non_regrid import get_source_filepaths_from_batch_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadf8cc-8625-402a-ad43-92dc94d0c0ad",
   "metadata": {},
   "source": [
    "### Check for completeness of regridded files\n",
    "\n",
    "Get a list of filepaths for all regridded files. This now includes files that were cropped and not regridded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a52bbd-e22d-43c3-a7a9-bf01fa4c7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "regrid_fps = list(regrid_dir.glob(\"**/*.nc\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc70ce-3801-4b28-97ea-28c843180bc7",
   "metadata": {},
   "source": [
    "Check that all files selected for regridding (which are those listed in the batch regrid files) are found in the regrid directory on scratch space. Not all of them will be found, as most will have been split into yearly files (daily frequency) or grouped into century long files (monthly frequency). So we will need to recreate the expected filenames from these source files.\n",
    "\n",
    "First, need to get all of the source filenames from the batch files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d73eede-9882-4442-ace1-f005a18c85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_fps = get_source_filepaths_from_batch_files(regrid_batch_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ca5d6-0a60-4ebb-876e-5beba0c3f776",
   "metadata": {},
   "source": [
    "Since we renamed the files by replacing the grid type component of the original filename with \"regrid\" upon saving the regridded files, we must do this again to compare the source file names with the regridded filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ba9782",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_fns = set([generate_regrid_filepath(fp, regrid_dir).name for fp in src_fps])\n",
    "regrid_fns = set([fp.name for fp in regrid_fps])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fa88c",
   "metadata": {},
   "source": [
    "Now split the source filenames up to yearly filenames for those that aren't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27cf5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_src_filename_to_regrid_filenames(fn):\n",
    "    timeframe = fn.split(\"_\")[-1].split(\".nc\")[0]\n",
    "    tmp_fn = \"_\".join(fn.split(\"_\")[:-1]) + \"_{}.nc\"\n",
    "    freq = fn.split(\"_\")[1]\n",
    "    new_time_strings = []\n",
    "    if \"day\" in freq:\n",
    "        start_date, end_date = pd.to_datetime(timeframe.split(\"-\"))\n",
    "        # handle cases where file starts on last day of year (those were simply dropped in regridding)\n",
    "        if start_date.month == 12:\n",
    "            start_date = pd.to_datetime(f\"{start_date.year + 1}0101\")\n",
    "        # handle cases where file ends on first day of year (also dropped in regridding)\n",
    "        if end_date.month == 1:\n",
    "            end_date = pd.to_datetime(f\"{end_date.year - 1}1231\")\n",
    "        for year in range(start_date.year, end_date.year + 1):\n",
    "            new_time_strings.append(f\"{year}0101-{year}1231\")\n",
    "    elif \"mon\" in freq:\n",
    "        if len(timeframe.split(\"-\")[0]) == 8:\n",
    "            start_date, end_date = pd.to_datetime(timeframe.split(\"-\"))\n",
    "        else:\n",
    "            start_date, end_date = pd.to_datetime(\n",
    "                [x + \"01\" for x in timeframe.split(\"-\")]\n",
    "            )\n",
    "        if start_date.month == 12:\n",
    "            start_date = pd.to_datetime(f\"{start_date.year + 1}01\")\n",
    "        for year in range(start_date.year, end_date.year + 1):\n",
    "            new_time_strings.append(f\"{year}01-{year}12\")\n",
    "\n",
    "    dst_fns = [tmp_fn.format(time_str) for time_str in new_time_strings]\n",
    "\n",
    "    return dst_fns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80069822",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_regrid_fns = []\n",
    "# [src_regrid_fns.extend(convert_src_filename_to_regrid_filenames(fn)) for fn in src_fns]\n",
    "for fn in src_fns:\n",
    "    src_regrid_fns.extend(convert_src_filename_to_regrid_filenames(fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5f52e-721d-4da5-9725-ade670d3eb42",
   "metadata": {},
   "source": [
    "Now, the source files which are not found in the regridding output directory can be isolated. There should be no such files because they should have all been regridded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6ec1e3-0b0c-4c0f-ae53-f393b3feea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fns = list(set(src_regrid_fns) - set(regrid_fns))\n",
    "assert len(missing_fns) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57dbe94-055e-4631-93bf-e43d52b41f0c",
   "metadata": {},
   "source": [
    "Sometimes the processing code would create files and fail before writing them completely. Ensure there are no files smaller than 1 MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "037cc7c3-5909-495f-a60f-5b7b6cfee4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 286 ms, sys: 124 ms, total: 411 ms\n",
      "Wall time: 9min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def is_smol_file(fp):\n",
    "    \"\"\"Check whether a file is small for a regridded CMIP6 file.\"\"\"\n",
    "    if fp.stat().st_size / (1e3 ** 2) < 0.5:\n",
    "        return fp\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "with Pool(8) as pool:\n",
    "    smol_fps = pool.map(is_smol_file, regrid_fps)\n",
    "    \n",
    "smol_fps = [fp for fp in smol_fps if fp is not None]\n",
    "\n",
    "assert len(smol_fps) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c909629-b67c-49fd-87e5-8f4421a77d39",
   "metadata": {},
   "source": [
    "### Validate regridding\n",
    "\n",
    "Verify that all regridded files (including those that were simply cropped) all have the target grid by checking that the latitude and longitude variables match those of the file used for the target grid.\n",
    "\n",
    "Define a function to check that the lat and lon arrays of the target grid match those of a given regridded fielpath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f44741fa-9cad-4d66-b241-64d6f99b654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_latlon(args):\n",
    "    regrid_fp, target_lat_arr, target_lon_arr = args\n",
    "    regrid_ds = xr.open_dataset(regrid_fp)\n",
    "\n",
    "    try:\n",
    "        assert regrid_ds[\"lat\"].values.shape == target_lat_arr.shape\n",
    "        assert regrid_ds[\"lon\"].values.shape == target_lon_arr.shape\n",
    "        assert np.all(regrid_ds[\"lat\"].values == target_lat_arr)\n",
    "        assert np.all(regrid_ds[\"lon\"].values == target_lon_arr)\n",
    "        result = None\n",
    "    except AssertionError:\n",
    "        result = regrid_fp\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d9a06-7a88-46a4-bc19-e89d7698d9ec",
   "metadata": {},
   "source": [
    "Run the check for all regridded and cropped files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e47979-7232-41d1-86af-ab0c7d6ca077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157160/157160 [38:21<00:00, 68.28it/s] \n"
     ]
    }
   ],
   "source": [
    "dst_ds = open_and_crop_dataset(target_grid_fp, lat_slice=slice(50, 90))\n",
    "target_lat_arr = dst_ds[\"lat\"].values\n",
    "target_lon_arr = dst_ds[\"lon\"].values\n",
    "\n",
    "args = [(fp, target_lat_arr, target_lon_arr) for fp in regrid_fps]\n",
    "\n",
    "results = []\n",
    "with Pool(8) as pool:\n",
    "    # _ = pool.starmap(validate_latlon, args)\n",
    "    for arg in tqdm.tqdm(pool.imap_unordered(validate_latlon, args), total=len(args)):\n",
    "        results.append(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a76ec5-f83a-48e0-a843-26da85fd242f",
   "metadata": {},
   "source": [
    "If no filenames were returned from the above function, then all of the files in the regrid output directory do indeed have the same latitude and longitude grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b29494-46ad-410a-ab3a-db21a6cebc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all([x is None for x in results])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
