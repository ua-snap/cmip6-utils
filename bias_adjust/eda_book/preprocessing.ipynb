{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17e7470",
   "metadata": {},
   "source": [
    "# Preprocessing for downscaling exploratory book\n",
    "\n",
    "This notebook serves as a single place for long running operations that may need some babysitting that produce outputs to be used in other notebooks in this exploratory project.\n",
    "\n",
    "It includes the following pieces:\n",
    "\n",
    "\n",
    "1. Regridding historical CMIP6 precipitation data to the target 4km ERA5 grid in 3338 via conservative interpolation and bias-adjustment via quantile delta mapping. \n",
    "\n",
    "   * Done here because the CMIP6 data available for exploring the bias-adjustment have only been regridded via bilinear interpolation, and because the current regridding pipeline does not currently (April 2025) allow conservative regridding \n",
    "   * This also includes processing of the indicators for the downscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config cell\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "from xclim import units\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import dask\n",
    "import baeda\n",
    "from xclim import units, sdba\n",
    "\n",
    "\n",
    "dask.config.set({\"large-graph-warning-threshold\": \"100MB\"})\n",
    "\n",
    "# target grid file\n",
    "target_grid_file = Path(\"/beegfs/CMIP6/kmredilla/downscaling/era5_target_slice.nc\")\n",
    "\n",
    "# cmip6 dir\n",
    "cmip6_dir = Path(\"/beegfs/CMIP6/arctic-cmip6/CMIP6\")\n",
    "\n",
    "era5_dir = Path(\"/center1/CMIP6/kmredilla/era5_zarr\")\n",
    "\n",
    "\n",
    "# tmp dir for writing inputs/outputs\n",
    "tmp_dir = Path(\"/beegfs/CMIP6/kmredilla/downscaling/eda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1eda25",
   "metadata": {},
   "source": [
    "## 1. Downscaling GFDL-ESM4 with conservative regridding and QDM adjustment\n",
    "\n",
    "Setup - get the output filepaths and spin up a cluster (makes a big cluster for fast compute!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44be53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmredilla/miniconda3/envs/cmip6-utils/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 42605 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tmp_regrid_fn = \"{var_id}_{model}_{scenario}_regrid_{interp_method}.zarr\"\n",
    "tmp_adj_fn = \"{var_id}_{model}_{scenario}_adj_{interp_method}.zarr\"\n",
    "\n",
    "# regridded path\n",
    "hist_regrid_path = tmp_dir.joinpath(\n",
    "    tmp_regrid_fn.format(\n",
    "        var_id=\"pr\",\n",
    "        model=\"GFDL-ESM4\",\n",
    "        scenario=\"historical\",\n",
    "        interp_method=\"conservative\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# adjusted path\n",
    "hist_adj_path = tmp_dir.joinpath(\n",
    "    tmp_adj_fn.format(\n",
    "        var_id=\"pr\",\n",
    "        model=\"GFDL-ESM4\",\n",
    "        scenario=\"historical\",\n",
    "        interp_method=\"conservative\",\n",
    "    )\n",
    ")\n",
    "# for interactive nb testing\n",
    "cluster = SLURMCluster(\n",
    "    cores=28,\n",
    "    processes=14,\n",
    "    # n_workers=14,\n",
    "    memory=\"128GB\",\n",
    "    # queue=\"debug\",\n",
    "    queue=\"t2small\",\n",
    "    # walltime=\"01:00:00\",\n",
    "    walltime=\"12:00:00\",\n",
    "    log_directory=\"/beegfs/CMIP6/kmredilla/tmp\",\n",
    "    account=\"cmip6\",\n",
    "    interface=\"ib0\",\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "cluster.scale(n=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b55967",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_id = \"pr\"\n",
    "\n",
    "# load and prep data for regridding\n",
    "hist_fps = [\n",
    "    list(\n",
    "        cmip6_dir.glob(\n",
    "            f\"CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/day/{var_id}/gr1/v20190726/{var_id}_day_GFDL-ESM4_historical_r1i1p1f1_gr1_{year}0101-*1231.nc\"\n",
    "        )\n",
    "    )[0]\n",
    "    for year in [1950, 1970, 1990, 2010]\n",
    "]\n",
    "hist_ds = xr.open_mfdataset(hist_fps, parallel=True, engine=\"h5netcdf\")\n",
    "\n",
    "# other datasets do not have time as a dimension for the bounds variables!\n",
    "# GFDL-ESM4 has this wrong I believe. Should be no need for time as a dimension of bnds variables. Grid should be fixed through time.\n",
    "# need to rectify this for GFDL-ESM4, as we get an assertion error in the underlying cf_xarray accessor get_bounds_dim_name\n",
    "hist_ds[\"lon_bnds\"] = hist_ds.lon_bnds.isel(time=0, drop=True)\n",
    "hist_ds[\"lat_bnds\"] = hist_ds.lat_bnds.isel(time=0, drop=True)\n",
    "\n",
    "# load target\n",
    "target_ds = xr.open_dataset(target_grid_file, engine=\"h5netcdf\")\n",
    "\n",
    "# add bounds variables (needed for consvervative regridding)\n",
    "target_ds = target_ds.cf.add_bounds(\"lon\")\n",
    "target_ds = target_ds.cf.add_bounds(\"lat\")\n",
    "\n",
    "# Initialize regridder\n",
    "regridder = xe.Regridder(\n",
    "    hist_ds,\n",
    "    target_ds,\n",
    "    \"conservative\",\n",
    "    unmapped_to_nan=True,\n",
    "    periodic=True,\n",
    "    ignore_degenerate=True,\n",
    ")\n",
    "hist_regrid_ds = regridder(hist_ds, keep_attrs=True)\n",
    "\n",
    "\n",
    "# Prep the data for downscaling before writing to zarr, including chunking:\n",
    "def prep_regrid(regrid_ds):\n",
    "    var_id = list(regrid_ds.data_vars)[0]\n",
    "    target_unit = baeda.units_lu[var_id]\n",
    "    sim = baeda.drop_non_coord_vars(regrid_ds)[var_id]\n",
    "    sim = units.convert_units_to(\n",
    "        sim.assign_coords(time=sim.time.dt.floor(\"D\")), target_unit\n",
    "    )\n",
    "\n",
    "    return sim\n",
    "\n",
    "\n",
    "hist = prep_regrid(hist_regrid_ds)\n",
    "\n",
    "# subset time\n",
    "hist = hist.sel(time=slice(\"1965-01-01\", \"2014-12-31\"))\n",
    "\n",
    "# chunk em\n",
    "chunk_kwargs = {\"time\": -1, \"x\": 10, \"y\": 10}\n",
    "hist = hist.chunk(**chunk_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9104549",
   "metadata": {},
   "source": [
    "Write to Zarr, triggering computation. This should help with later `dask`-ing of adjustment. We can have a dask array of regridded data instead of a dask array that includes the regrid operation, minimizing the task graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmredilla/miniconda3/envs/cmip6-utils/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 47.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if hist_regrid_path.exists():\n",
    "    shutil.rmtree(hist_regrid_path)\n",
    "\n",
    "_ = hist.to_dataset().to_zarr(hist_regrid_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb13631",
   "metadata": {},
   "source": [
    "Now run the bias-adjustment.\n",
    "\n",
    "Close previous stuff and connect to the new zarr store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ds.close()\n",
    "del hist_ds\n",
    "\n",
    "hist_ds = xr.open_zarr(hist_regrid_path)\n",
    "\n",
    "# connect to ERA5 data\n",
    "era5_stores = {\n",
    "    # \"t2max\": era5_dir.joinpath(\"t2max_era5.zarr\"),\n",
    "    \"pr\": era5_dir.joinpath(\"pr_era5.zarr\"),\n",
    "}\n",
    "era5_ds = baeda.open_era5_dataset(era5_stores)\n",
    "ref = era5_ds[var_id].chunk(chunk_kwargs)\n",
    "\n",
    "# QDM: train the adjustment\n",
    "train_kwargs = dict(\n",
    "    ref=ref,\n",
    "    hist=hist_ds[var_id],\n",
    "    nquantiles=50,\n",
    "    group=\"time.dayofyear\",\n",
    "    window=31,\n",
    "    kind=baeda.varid_adj_kind_lu[var_id],\n",
    ")\n",
    "if var_id in baeda.adapt_freq_thresh_lu:\n",
    "    train_kwargs.update(\n",
    "        adapt_freq_thresh=baeda.adapt_freq_thresh_lu[var_id],\n",
    "        jitter_under_thresh_value=baeda.jitter_under_thresh_lu[var_id],\n",
    "    )\n",
    "\n",
    "qdm_train = sdba.QuantileDeltaMapping.train(**train_kwargs)\n",
    "\n",
    "# set up the QDM adjustment for historical\n",
    "hist_adj = qdm_train.adjust(\n",
    "    hist_ds[var_id],\n",
    "    extrapolation=\"constant\",\n",
    "    interp=\"nearest\",\n",
    ")\n",
    "hist_adj.name = var_id\n",
    "hist_adj = hist_adj.transpose(\"time\", \"y\", \"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f50e53",
   "metadata": {},
   "source": [
    "Write the adjusted (downscaled) data to Zarr, triggering the adjustment computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmredilla/miniconda3/envs/cmip6-utils/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 25.51 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if hist_adj_path.exists():\n",
    "    shutil.rmtree(hist_adj_path)\n",
    "\n",
    "_ = hist_adj.to_dataset().to_zarr(hist_adj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663c19d",
   "metadata": {},
   "source": [
    "Now run the indicators and write to zarr:\n",
    "\n",
    "(load the downscaled data first, seems to perform much better for this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to perform better if loaded?\n",
    "# this whole cell could take up to 10 minutes\n",
    "hist_adj_ds = xr.open_zarr(hist_adj_path)\n",
    "hist_adj_ds = hist_adj_ds.load()\n",
    "\n",
    "hist_idx = baeda.run_indicators(hist_adj_ds[var_id])\n",
    "\n",
    "hist_idx_path = tmp_dir.joinpath(\n",
    "    tmp_adj_fn.format(\n",
    "        var_id=var_id + \"idx\",\n",
    "        model=\"GFDL-ESM4\",\n",
    "        scenario=\"historical\",\n",
    "        interp_method=\"conservative\",\n",
    "    )\n",
    ")\n",
    "if hist_idx_path.exists():\n",
    "    shutil.rmtree(hist_idx_path)\n",
    "\n",
    "_ = hist_idx.chunk(**chunk_kwargs).to_zarr(hist_idx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfab549",
   "metadata": {},
   "source": [
    "Close the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fa24747",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda6d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4f548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
