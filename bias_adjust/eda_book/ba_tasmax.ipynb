{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias adjustment of maximum air temperature\n",
    "\n",
    "Here we will step through what we have learned in developing this workflow by performing a bias adjustment on maximum air temperature data using the exact method we plan to implement for all input data. (well, almost exact - there are some minor differences between adjusting $Pr$ and $T_{max}$ / $T_{\\Delta}$ is that we will discuss in the precipitation adjustment section).\n",
    "\n",
    "First, load the libs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from xclim import sdba\n",
    "from dask.distributed import Client\n",
    "from xclim.sdba.detrending import LoessDetrend\n",
    "import dask\n",
    "\n",
    "# we have to make some big chunks and this will silence a warning about that\n",
    "dask.config.set(**{\"array.slicing.split_large_chunks\": False})\n",
    "\n",
    "log_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have developed some functions that will help make file handling and other tasks easier. Their source can be viewed on **SOME PAGE**. They are saved in the `baeda` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baeda import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the dask client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "We will be using ERA5 data as our historical reference for adjusting data. For now, we will just go with the most recent 30 years of available data that we have available (available via SNAP infra), i.e. 1993 - 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"tas\" in ERA5 is t2m, so we have named the daily max version t2mmax\n",
    "ref_var_id = \"t2mmax\"\n",
    "ref_start_year = 1993\n",
    "ref_end_year = 2022\n",
    "ref_fps = get_era5_fps(ref_var_id, ref_start_year, ref_end_year)\n",
    "\n",
    "var_id = \"tasmax\"\n",
    "model = \"GFDL-ESM4\"\n",
    "hist_start_year = 1993\n",
    "hist_end_year = 2014\n",
    "hist_fps = get_cmip6_fps(model, \"historical\", var_id, hist_start_year, hist_end_year)\n",
    "\n",
    "scenario = \"ssp585\"\n",
    "sim_ref_start_year = 2015\n",
    "sim_ref_end_year = 2022\n",
    "sim_ref_fps = get_cmip6_fps(\n",
    "    model, scenario, var_id, sim_ref_start_year, sim_ref_end_year\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the datasets from the yearly data files for each of ERA5 and CMIP6:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ds = xr.open_mfdataset(hist_fps + sim_ref_fps)\n",
    "# convert calendar to noleap to match CMIP6\n",
    "ref_ds = xr.open_mfdataset(ref_fps).convert_calendar(\"noleap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Here is a snapshot of what we are working with. Historical reference (ERA5) on the left, modeled historical GCM on the right, for the same time step (i.e. same day).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "ref_ds[\"t2mmax\"].isel(time=1).plot(ax=axes[0])\n",
    "axes[0].set_title(\"Reference\")\n",
    "hist_ds[\"tasmax\"].isel(time=1).plot(ax=axes[1])\n",
    "axes[1].set_title(\"Historical simulated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "Next, we need to rechunk the time dimension into one chunk. The training functions in `xclim.sdba` will not work with datasets having multiple chunks along the adjustment dimension (time in this case). We split it up into chunks over the lat and lon dims for some added optimization (hopefully).\n",
    "\n",
    "We will also initialize the bias adjustment here. We are performing a \"detrended\" quantile mapping grouped by day of the year (you will also see this spelled as DOY or doy). To quote Lavoie et al 2024:\n",
    "\n",
    ">The procedure is univariate (applied to each variable individually), acts independently on the trends and the anomalies, and is applied iteratively on each day of the year as well as at each grid point.\n",
    "\n",
    "Other parameters can be seen below. Not sure what they all mean, but we are using 50 quantiles and a window of 31 days. specifying `d=0` uses \"local constancy\", meaning local estimates are weighted averages.\n",
    "\n",
    "The final adjusted output will be stored in `scen`, which is currently just a dask task graph until we call `.compute()` on it, or need the data in some way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = get_rechunked_da(ref_ds, ref_var_id)\n",
    "hist = get_rechunked_da(hist_ds, var_id)\n",
    "\n",
    "dqm = sdba.DetrendedQuantileMapping.train(\n",
    "    ref, hist, nquantiles=50, group=\"time.dayofyear\", window=31, kind=\"+\"\n",
    ")\n",
    "# Create the detrending object\n",
    "det = LoessDetrend(group=\"time.dayofyear\", d=0, niter=1, f=0.2, weights=\"tricube\")\n",
    "scen = dqm.adjust(hist, extrapolation=\"constant\", interp=\"nearest\", detrend=det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run it. For demonstration purposes, we will only run this adjustment for a single pixel initially:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_di = {\"lon\": -147, \"lat\": 65}\n",
    "scen = scen.sel(sel_di, method=\"nearest\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Now have a look at the adjusted data which have been saved in `scen`. We will use this kind of plot often to help evaluate the adjustment performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_ts(ref.sel(sel_di, method=\"nearest\"), hist.sel(sel_di, method=\"nearest\"), scen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "That is certainly an improvement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_cmip6_utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
