{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Bias adjusting precipitation data\n",
    "\n",
    "Following Lavoie et al 2024, the procedure for adjusting precipitation is similar to that for $T_{max}$ and $T_{\\Delta}$, but with two additional considerations: 1) matching dry day frequency, and 2) removing (substituting) zero values. \n",
    "\n",
    "From their \"Bias Adjustment of ESPO-G6-R2 v1.0.0\" document included in the [configuration repository](https://github.com/Ouranosinc/ESPO-G/blob/fbfc55c6378f009877ec35e58805b847d300d99b/documentation/ESPO_G6_R2v100_adjustment.pdf):\n",
    "\n",
    "> when the model has a higher dry-day frequency than the reference, the calibration step of the quantile mapping adjustment will incorrectly map all dry days to precipitation days, resulting in a wet bias. The frequency adaptation method finds the fraction of ”extra” dry days\n",
    "\n",
    "So we will implement this same \"frequency adaptation\" method by supplying an additional argument in the training function. (See [this part](https://xclim.readthedocs.io/en/stable/notebooks/sdba.html#First-example-:-pr-and-frequency-adaptation) of the example notebook in the `xclim` docs for a little more info). \n",
    "\n",
    "Removal of any zero values will be done by replacing them with random \"jitter\" very close to zero (see that linked document above for more on this).\n",
    "\n",
    "Perform the same setup steps as we did for the $T_{max}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from xclim import sdba\n",
    "from dask.distributed import Client\n",
    "from xclim import units\n",
    "from xclim.sdba.detrending import LoessDetrend\n",
    "import dask\n",
    "\n",
    "# we have to make some big chunks and this will silence a warning about that\n",
    "dask.config.set(**{\"array.slicing.split_large_chunks\": False})\n",
    "\n",
    "log_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the same functions and constants used for the $T_{max}$ adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baeda import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the dask client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be using ERA5 data as our historical reference, using 1993 - 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_var_id = \"tp\"\n",
    "ref_start_year = 1993\n",
    "ref_end_year = 2022\n",
    "ref_fps = get_era5_fps(ref_var_id, ref_start_year, ref_end_year)\n",
    "\n",
    "var_id = \"pr\"\n",
    "model = \"GFDL-ESM4\"\n",
    "hist_start_year = 1993\n",
    "hist_end_year = 2014\n",
    "hist_fps = get_cmip6_fps(model, \"historical\", var_id, hist_start_year, hist_end_year)\n",
    "\n",
    "scenario = \"ssp585\"\n",
    "sim_ref_start_year = 2015\n",
    "sim_ref_end_year = 2022\n",
    "sim_ref_fps = get_cmip6_fps(\n",
    "    model, scenario, var_id, sim_ref_start_year, sim_ref_end_year\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the datasets from the yearly data files for each of ERA5 and CMIP6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_ds = xr.open_mfdataset(hist_fps + sim_ref_fps)\n",
    "# convert calendar to noleap to match CMIP6\n",
    "# for some reason the precip data for 2022 has an expver variable while the t2m data doesnt\n",
    "# drop this as it might be causing problems\n",
    "ref_ds = (\n",
    "    xr.open_mfdataset(ref_fps)\n",
    "    .convert_calendar(\"noleap\")\n",
    "    .sel(expver=1)\n",
    "    .drop_vars(\"expver\")\n",
    ")\n",
    "\n",
    "ref = get_rechunked_da(ref_ds, ref_var_id)\n",
    "hist = get_rechunked_da(hist_ds, var_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set the units to `m d-1` - instead of current `m` - so it is compatible with xclim's (`pint`'s) unit scheme (since these are at a daily time step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.attrs[\"units\"] = \"m d-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure there are no true 0's in the data, `xscen` uses `xclim`'s `sdba.processing.jitter_under_thresh`, using a threshold of 0.01 mm / day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = sdba.processing.jitter_under_thresh(ref, thresh=\"0.01 mm d-1\")\n",
    "hist = sdba.processing.jitter_under_thresh(hist, thresh=\"0.01 mm d-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of time range just for examination\n",
    "before_jitter = ref_ds[\"tp\"].isel(time=slice(100, 200)).compute()\n",
    "after_jitter = ref.isel(time=slice(100, 200)).compute()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "before_jitter.plot.hist(\n",
    "    range=[-0.01, 0.01], alpha=0.75, bins=40, ax=ax, label=\"Before jitter\"\n",
    ")\n",
    "after_jitter.plot.hist(\n",
    "    range=[-0.01, 0.01], alpha=0.75, bins=40, ax=ax, label=\"After jitter\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there were some negative values in this model beforehand and those have been bounded at 0. You actually can't tell from this graph that those new values are indeed not 0, so here is proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before jitter minimum:\", before_jitter.min().values)\n",
    "print(\"After jitter minimum:\", after_jitter.min().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the training object.\n",
    "\n",
    "We will set the \"frequency adaptation\" threshold to 1 mm / d, again, following along with Lavoie et al:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqm = sdba.DetrendedQuantileMapping.train(\n",
    "    ref=ref,\n",
    "    hist=hist,\n",
    "    nquantiles=50,\n",
    "    group=\"time.dayofyear\",\n",
    "    window=31,\n",
    "    kind=\"*\",\n",
    "    adapt_freq_thresh=\"1 mm d-1\",\n",
    ")\n",
    "\n",
    "# Create the detrending object\n",
    "det = LoessDetrend(group=\"time.dayofyear\", d=0, niter=1, f=0.2, weights=\"tricube\")\n",
    "\n",
    "# now the adjustment object / task graph\n",
    "scen = dqm.adjust(hist, extrapolation=\"constant\", interp=\"nearest\", detrend=det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run it. We can again run it for a single pixel to save some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_di = {\"lon\": -147, \"lat\": 65}\n",
    "scen = scen.sel(sel_di, method=\"nearest\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some plotting to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model data to meters / day to match ref and scen\n",
    "hist_mpd = units.convert_units_to(hist.sel(sel_di, method=\"nearest\"), \"m d-1\")\n",
    "plot_avg_ts(ref.sel(sel_di, method=\"nearest\"), hist_mpd, scen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's actually tough to make sense of, although it does appear to be an improvement, with the adjusted data seemignly closer to the reference. Maybe monthly averages would be better to visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_ts(ref.sel(sel_di, method=\"nearest\"), hist_mpd, scen, gb_str=\"time.month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's definitely an improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
